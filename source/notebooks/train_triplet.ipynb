{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/source/main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import time\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from naruto_skills.new_voc import Voc\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model_def.simple_cnn import SimpleCNN\n",
    "from model_def.siamese_model_ import SiameseModel\n",
    "from model_def.siamese_core import SiameseModelCore\n",
    "from data_for_train.pool import PoolDocs\n",
    "from utils import pytorch_utils\n",
    "from preprocess import preprocessor\n",
    "from data_for_train.index_dataset import IndexDataset\n",
    "from data_for_train.positive_dataset import PositiveDataset\n",
    "from naruto_skills.training_checker import TrainingChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2input_tensors(docs, device):\n",
    "    preprocessed_docs = [preprocessor.infer_preprocess(doc) for doc in docs]\n",
    "#     max_len = max([len(item.split()) for item in preprocessed_docs])\n",
    "    max_len = 100\n",
    "    preprocessed_docs = [' '.join(doc.split()[:max_len]) for doc in preprocessed_docs]\n",
    "    word_input = voc.docs2idx(preprocessed_docs, equal_length=max_len)\n",
    "    inputs = np.array(word_input)\n",
    "    input_tensors = torch.from_numpy(inputs)\n",
    "    input_tensors = input_tensors.to(device)\n",
    "    return input_tensors\n",
    "\n",
    "def predict_batch(docs):\n",
    "    with torch.no_grad():\n",
    "        input_tensors = docs2input_tensors(docs, device)\n",
    "        predict_tensor = model(input_tensors)\n",
    "        predict_np = predict_tensor.cpu().numpy()\n",
    "        return predict_np[:, 1]\n",
    "\n",
    "def predict_docs(docs, batch_size):\n",
    "    return list(chain(*[predict_batch(docs[i: i+batch_size]) for i in tqdm(range(0, len(docs), batch_size))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(list_data):\n",
    "    \"\"\"\n",
    "    shape == (batch_size, col1, col2, ...)\n",
    "    \"\"\"\n",
    "    data = zip(*list_data)\n",
    "    data = [np.stack(col, axis=0) for col in data]\n",
    "    data = [torch.from_numpy(col) for col in data]\n",
    "    return data\n",
    "voc = Voc.load('/source/main/vocab/output/voc.pkl')\n",
    "MAX_LENGTH = 100\n",
    "BATCH_SIZE = 256\n",
    "EXP_ID = '22a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_neg = pd.read_csv('/source/main/data_for_train/output/huge_pool/wiki.csv', nrows=1e6, usecols=['target'])\n",
    "# df_neg.rename(columns={'target': 'mention'}, inplace=True)\n",
    "# df_neg.dropna(inplace=True, subset=['mention'])\n",
    "# df_neg.drop_duplicates(inplace=True, subset=['mention'])\n",
    "# df_neg = df_neg.iloc[:794323, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_neg.shape)\n",
    "# neg = IndexDataset(voc, list(df_neg['mention']), equal_length=MAX_LENGTH)\n",
    "# neg = PoolDocs(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool = pd.read_csv('/source/main/data_for_train/output/train/pool.csv', nrows=1e6)\n",
    "df_pool.dropna(inplace=True, subset=['mention'])\n",
    "df_pool.drop_duplicates(inplace=True, subset=['mention'])\n",
    "df_pool = df_pool.iloc[:794323, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(794323, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_pool.shape)\n",
    "pool = IndexDataset(voc, list(df_pool['mention']), equal_length=MAX_LENGTH)\n",
    "# pool = PoolDocs(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "794323"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Positive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33539, 15)\n"
     ]
    }
   ],
   "source": [
    "POSITIVE_NAME = 'positive_class_9'\n",
    "df_pos = pd.read_csv('/source/main/data_for_train/output/train/%s.csv' % POSITIVE_NAME)\n",
    "df_pos.dropna(inplace=True, subset=['mention'])\n",
    "df_pos.drop_duplicates(inplace=True, subset=['mention'])\n",
    "\n",
    "print(df_pos.shape)\n",
    "pos = IndexDataset(voc, list(df_pos['mention']), equal_length=MAX_LENGTH)\n",
    "# pos = PositiveDataset(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool_data, _ = zip(*pool)\n",
    "# pool_data = [str(item) for item in pool_data]\n",
    "# pos_data, _ = zip(*pos)\n",
    "# pos_data = [str(item) for item in pos_data]\n",
    "# len(set(pool_data).intersection(set(pos_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33539, 15)\n"
     ]
    }
   ],
   "source": [
    "df_anchor = pd.read_csv('/source/main/data_for_train/output/train/%s.csv' % POSITIVE_NAME)\n",
    "df_anchor.dropna(inplace=True, subset=['mention'])\n",
    "df_anchor.drop_duplicates(inplace=True, subset=['mention'])\n",
    "df_anchor = df_anchor.sample(df_anchor.shape[0], random_state=43)\n",
    "print(df_anchor.shape)\n",
    "anchor = IndexDataset(voc, list(df_anchor['mention']), equal_length=MAX_LENGTH)\n",
    "# anchor = PositiveDataset(anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, anchor, pos, pool):\n",
    "        super(TripletDataset, self).__init__()\n",
    "        self.anchor = anchor\n",
    "        self.pos = pos\n",
    "        self.pool = pool\n",
    "        self.len_pos = len(pos)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pool)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.anchor[idx % self.len_pos], self.pos[idx % self.len_pos], self.pool[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TripletDataset(anchor, pos, pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset=ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bé mình uống nan __d__ năm rồi , thấy khỏe mạnh và phát triển tốt lắm . mình hoàn toàn yên tâm về nan',\n",
       " 'sữa friso mát lắm , bé uống thích lắm',\n",
       " 'trần hồng quí mom cứ chịu khó nấu , con ăn đc bao nhiêu thì ăn thôi mom , ko ép , để cho chơi nhiều và giảm cử ăn vặt là mấy ảnh sẽ chịu ăn nhiều hơn thôi . bé nhà mình có dùng thêm pediasure , uống ngày __d__ cử thấy con ăn uống ngon miệng hơn nhiều ấy mom .']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.idx2docs(data[0].cpu().numpy()[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trộm vía con tiêu hóa tốt nên thích lắm vì uống sữa nan đó chị',\n",
       " 'bé em cũng dùng friso ne , vậy mà giờ mới biết đến cuộc thi',\n",
       " 'optimum mình dùng mấy hộp ko tăng lạng nào luôn']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.idx2docs(data[1].cpu().numpy()[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loa loa __o__ ... aaaaaa ngon tuyệt vời mn ơiii 😭😭😭 sấu ngâm mắm gừng ớt 😻😻😻 bác e làm đảm bảo sạch sẽ ngon tuyệt vời luôn ấy huhu nước mắm nam ngư đun lên , sấu cũng trần qua nước sôi để k bị váng và để được __o__ sạch sẽ vừa ngon luôn __o__ hộp đầy như kia __d__k nha mn 😋😋😋 k ăn thì sẽ tiếc lắm ấy ... ăn ngấm vào tận trong hột sấu luôn = ) ) ) ) bác nào ăn báo e luôn nàoooo __o__ sđt __d__ hoặc __d__',\n",
       " 'lê hà trang tiếc cái t k dùng kotex',\n",
       " 'tại sao lại như vậy ?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.idx2docs(data[2].cpu().numpy()[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_model = SiameseModelCore(voc.get_embedding_weights())\n",
    "model = SiameseModel(core_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE_TRAINED_MODEL='/source/main/train/output/saved_models/12bb/None.pt'\n",
    "# checkpoint = torch.load(PRE_TRAINED_MODEL, map_location=device)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13886910"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_utils.count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs):\n",
    "    model.train()\n",
    "    step_loss = model.train_batch(inputs[0], inputs[1], inputs[2])\n",
    "    return step_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3103"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# # import pdb; pdb.set_trace()\n",
    "# predict_docs(['giá bao tiền', 'ee', 'Giảm giá sốc'], batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = docs2input_tensors(['giá bao tiền'], device)\n",
    "pos = docs2input_tensors(['bao nhiêu thế ?'], device)\n",
    "neg = docs2input_tensors(['hôm nay tôi đi học'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dis_1 tensor([0.0874], device='cuda:0', grad_fn=<NormBackward1>)\n",
      "dis_2 tensor([0.2014], device='cuda:0', grad_fn=<NormBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print('dis_1', model.get_distance(anchor, pos))\n",
    "print('dis_2', model.get_distance(anchor, neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]INFO:root:\t Step: 0 Loss: 10.00150 \n",
      "100it [01:12,  1.39it/s]INFO:root:\t Step: 100 Loss: 0.72637 \n",
      "200it [02:24,  1.39it/s]INFO:root:\t Step: 200 Loss: 1.23698 \n",
      "300it [03:36,  1.39it/s]INFO:root:\t Step: 300 Loss: 0.51996 \n",
      "400it [04:48,  1.39it/s]INFO:root:\t Step: 400 Loss: 0.27985 \n",
      "500it [06:00,  1.39it/s]INFO:root:\t Step: 500 Loss: 0.49068 \n",
      "600it [07:12,  1.39it/s]INFO:root:\t Step: 600 Loss: 0.60918 \n",
      "700it [08:24,  1.39it/s]INFO:root:\t Step: 700 Loss: 0.17859 \n",
      "800it [09:36,  1.39it/s]INFO:root:\t Step: 800 Loss: 0.13675 \n",
      "900it [10:48,  1.38it/s]INFO:root:\t Step: 900 Loss: 0.14004 \n",
      "1000it [12:03,  1.39it/s]INFO:root:\t Step: 1000 Loss: 0.26835 \n",
      "1100it [13:15,  1.39it/s]INFO:root:\t Step: 1100 Loss: 0.38971 \n",
      "1200it [14:28,  1.38it/s]INFO:root:\t Step: 1200 Loss: 0.37492 \n",
      "1300it [15:40,  1.38it/s]INFO:root:\t Step: 1300 Loss: 0.22061 \n",
      "1400it [16:52,  1.39it/s]INFO:root:\t Step: 1400 Loss: 0.40308 \n",
      "1500it [18:19,  1.10s/it]INFO:root:\t Step: 1500 Loss: 0.08176 \n",
      "1600it [20:08,  1.09s/it]INFO:root:\t Step: 1600 Loss: 0.12635 \n",
      "1700it [21:27,  1.39it/s]INFO:root:\t Step: 1700 Loss: 0.20573 \n",
      "1800it [22:40,  1.38it/s]INFO:root:\t Step: 1800 Loss: 0.18790 \n",
      "1900it [24:01,  1.05s/it]INFO:root:\t Step: 1900 Loss: 0.16665 \n",
      "2000it [25:37,  1.39it/s]INFO:root:\t Step: 2000 Loss: 0.22034 \n",
      "2065it [26:24,  1.39it/s]"
     ]
    }
   ],
   "source": [
    "model.build_stuff_for_training(device)\n",
    "for epoch_idx in range(1):\n",
    "    start = time.time()\n",
    "    for idx, inputs in tqdm(enumerate(data_loader)):\n",
    "        inputs = [i.to(device) for i in inputs]\n",
    "        l = train_step(inputs)\n",
    "        if idx % 100 == 0:\n",
    "            logging.info('\\t Step: %s Loss: %.5f ', idx, l)\n",
    "    duration = time.time() - start\n",
    "    logging.info('Epoch %s took %.2f s', epoch_idx, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch_idx in range(10):\n",
    "#     start = time.time()\n",
    "#     model.train()\n",
    "#     for idx, inputs in tqdm(enumerate(data_loader)):    \n",
    "#         inputs = [i.to(device) for i in inputs]\n",
    "#         l = train_step(inputs)\n",
    "#     duration = time.time() - start\n",
    "#     logging.info('Epoch %s took %.2f s', epoch_idx, duration)\n",
    "    \n",
    "#     model.eval()    \n",
    "#     df_pos['pred'] = predict_docs(df_pos['mention'], batch_size=256)\n",
    "#     df_pool['pred'] = predict_docs(df_pool['mention'], batch_size=256)\n",
    "    \n",
    "#     logging.info('Recall: %s/%s=%.4f', (df_pos['pred']>=0.5).sum(), df_pos.shape[0], \n",
    "#                  (df_pos['pred']>=0.5).sum()/df_pos.shape[0])\n",
    "#     logging.info('Ratio on pool: %s/%s=%.4f', (df_pool['pred']>=0.5).sum(), df_pool.shape[0], \n",
    "#                  (df_pool['pred']>=0.5).sum()/df_pool.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "# ax = fig.add_subplot(1, 2, 1)\n",
    "# df_pos.loc[:500, 'pred'].hist(bins=100, ax=ax)\n",
    "# ax.set_title('Spy')\n",
    "# ax.set_xlim(0, 0.9)\n",
    "# ax.set_ylim(0, 100)\n",
    "\n",
    "# ax = fig.add_subplot(1, 2, 2)\n",
    "# df_pos.loc[500:, 'pred'].hist(bins=100, ax=ax)\n",
    "# ax.set_title('Positive')\n",
    "# ax.set_xlim(0, 0.9)\n",
    "# ax.set_ylim(0, 100)\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pool_social = pd.read_csv('/source/main/data_for_train/output/train/pool.csv')\n",
    "# df_pool_social.rename(columns={'target': 'mention'}, inplace=True)\n",
    "# df_pool_social.dropna(inplace=True, subset=['mention'])\n",
    "# df_pool_social.drop_duplicates(inplace=True, subset=['mention'])\n",
    "# df_pool_social = df_pool_social.iloc[:794323, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_checker = TrainingChecker(model, root_dir='/source/main/train/output/saved_models/%s/' % EXP_ID,\n",
    "                                   init_score=-10000)\n",
    "training_checker.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = docs2input_tensors([df_pos['mention'].iloc[20]], device)\n",
    "pos = docs2input_tensors([df_pos['mention'].iloc[40]], device)\n",
    "neg = docs2input_tensors(['hôm nay tôi đi học'], device)\n",
    "print('dis_1', model.get_distance(anchor, pos))\n",
    "print('dis_2', model.get_distance(anchor, neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos['mention'].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS = list(df_pos['mention'].sample(5))\n",
    "\n",
    "def get_distance(docs_1, docs_2):\n",
    "    with torch.no_grad():\n",
    "        docs1 = docs2input_tensors(docs_1, device)\n",
    "        docs2 = docs2input_tensors(docs_2, device)\n",
    "        return model.get_distance(docs1, docs2).cpu().numpy()\n",
    "\n",
    "def predict_batch(docs):\n",
    "    len_docs = len(docs)\n",
    "    pos_docs = [item for doc in POS for item in [doc]*len_docs]\n",
    "    docs = docs * len(POS)\n",
    "    \n",
    "    dis = np.array(get_distance(pos_docs, docs))\n",
    "    dis = dis.reshape((len(POS), len_docs))\n",
    "#     print(dis)\n",
    "    return dis.mean(axis=0)\n",
    "\n",
    "# def get_distance_to_pos(docs, batch_size):\n",
    "#     return list(chain(*[predict_batch(docs[i: i+batch_size]) for i in tqdm(range(0, len(docs), batch_size))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predict_docs(['giá bao tiền', 'ee', 'Giảm giá sốc'], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from sklearn import metrics\n",
    "\n",
    "from data_for_train.index_dataset import IndexDataset\n",
    "from data_for_train.positive_dataset import PositiveDataset\n",
    "from data_for_train import pool\n",
    "from naruto_skills.new_voc import Voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos['pred'] = predict_docs(list(df_pos['mention']), batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos['pred'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = df_pos['pred'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_eval = pd.read_csv('/source/main/data_for_train/output/eval/%s.csv' % POSITIVE_NAME)\n",
    "df_pos_eval = df_pos_eval.drop_duplicates(subset=['mention'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_eval['mention'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_eval.dropna(subset=['mention'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_eval['pred'] = predict_docs(list(df_pos_eval['mention']), batch_size=256)\n",
    "# print(sum(df_pos_eval['pred']>=0.5)/df_pos_eval.shape[0])\n",
    "# print(df_pos_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_pos_eval['pred']<=THRESHOLD).sum()/df_pos_eval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_test = pd.read_csv('/source/main/data_for_train/output/test/%s.csv' % POSITIVE_NAME)\n",
    "df_pos_test = df_pos_test.drop_duplicates(subset=['mention'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_test['pred'] = predict_docs(list(df_pos_test['mention']), batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(df_pos_test['pred']<=THRESHOLD)/df_pos_test.shape[0])\n",
    "print(df_pos_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score: pr/P(y=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool_eval = pd.read_csv('/source/main/data_for_train/output/eval/pool.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool_eval['pred'] = predict_docs(list(df_pool_eval['mention']), batch_size=256)\n",
    "\n",
    "# print(sum(df_pool_eval['pred']>=0.5)/df_pool_eval.shape[0])\n",
    "# print(df_pool_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_pool_eval['pred']<=THRESHOLD)/df_pool_eval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pool_eval['pred2'] = predict_docs(list(df_pool_eval['mention']), batch_size=64)\n",
    "# sum(df_pool_eval['pred2'] <= 5.871464)/df_pool_eval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(df_pool_eval['pred2'] <= 5.871464)/df_pool_eval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool_eval[df_pool_eval['pred']<=THRESHOLD].sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool_test = pd.read_csv('/source/main/data_for_train/output/test/pool.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool_test['pred'] = predict_docs(list(df_pool_test['mention']), batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(df_pool_test['pred']<=THRESHOLD)/df_pool_test.shape[0])\n",
    "print(df_pool_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pos_pred = sum(df_pool_test['pred']<=THRESHOLD)\n",
    "df_pool_test[df_pool_test['pred']<=THRESHOLD].sample(min(100, total_pos_pred)).to_csv('%s.csv' % EXP_ID, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pos_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos[['mention']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool_test[df_pool_test['pred']<=THRESHOLD].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
