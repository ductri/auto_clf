{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/source/main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import time\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from naruto_skills.new_voc import Voc\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model_def.simple_cnn import SimpleCNN\n",
    "from model_def.siamese_model_ import SiameseModel\n",
    "from model_def.siamese_core import SiameseModelCore\n",
    "from data_for_train.pool import PoolDocs\n",
    "from utils import pytorch_utils\n",
    "from preprocess import preprocessor\n",
    "from data_for_train.index_dataset import IndexDataset\n",
    "from data_for_train.positive_dataset import PositiveDataset\n",
    "from naruto_skills.training_checker import TrainingChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2input_tensors(docs, device):\n",
    "    preprocessed_docs = [preprocessor.infer_preprocess(doc) for doc in docs]\n",
    "#     max_len = max([len(item.split()) for item in preprocessed_docs])\n",
    "    max_len = 100\n",
    "    preprocessed_docs = [' '.join(doc.split()[:max_len]) for doc in preprocessed_docs]\n",
    "    word_input = voc.docs2idx(preprocessed_docs, equal_length=max_len)\n",
    "    inputs = np.array(word_input)\n",
    "    input_tensors = torch.from_numpy(inputs)\n",
    "    input_tensors = input_tensors.to(device)\n",
    "    return input_tensors\n",
    "\n",
    "def predict_batch(docs):\n",
    "    with torch.no_grad():\n",
    "        input_tensors = docs2input_tensors(docs, device)\n",
    "        predict_tensor = model(input_tensors)\n",
    "        predict_np = predict_tensor.cpu().numpy()\n",
    "        return predict_np[:, 1]\n",
    "\n",
    "def predict_docs(docs, batch_size):\n",
    "    return list(chain(*[predict_batch(docs[i: i+batch_size]) for i in tqdm(range(0, len(docs), batch_size))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(list_data):\n",
    "    \"\"\"\n",
    "    shape == (batch_size, col1, col2, ...)\n",
    "    \"\"\"\n",
    "    data = zip(*list_data)\n",
    "    data = [np.stack(col, axis=0) for col in data]\n",
    "    data = [torch.from_numpy(col) for col in data]\n",
    "    return data\n",
    "voc = Voc.load('/source/main/vocab/output/voc.pkl')\n",
    "MAX_LENGTH = 100\n",
    "BATCH_SIZE = 256\n",
    "EXP_ID = '22a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_neg = pd.read_csv('/source/main/data_for_train/output/huge_pool/wiki.csv', nrows=1e6, usecols=['target'])\n",
    "# df_neg.rename(columns={'target': 'mention'}, inplace=True)\n",
    "# df_neg.dropna(inplace=True, subset=['mention'])\n",
    "# df_neg.drop_duplicates(inplace=True, subset=['mention'])\n",
    "# df_neg = df_neg.iloc[:794323, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_neg.shape)\n",
    "# neg = IndexDataset(voc, list(df_neg['mention']), equal_length=MAX_LENGTH)\n",
    "# neg = PoolDocs(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool = pd.read_csv('/source/main/data_for_train/output/train/pool.csv', nrows=1e6)\n",
    "df_pool.dropna(inplace=True, subset=['mention'])\n",
    "df_pool.drop_duplicates(inplace=True, subset=['mention'])\n",
    "df_pool = df_pool.iloc[:794323, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(794323, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_pool.shape)\n",
    "pool = IndexDataset(voc, list(df_pool['mention']), equal_length=MAX_LENGTH)\n",
    "# pool = PoolDocs(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "794323"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Positive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33539, 15)\n"
     ]
    }
   ],
   "source": [
    "POSITIVE_NAME = 'positive_class_9'\n",
    "df_pos = pd.read_csv('/source/main/data_for_train/output/train/%s.csv' % POSITIVE_NAME)\n",
    "df_pos.dropna(inplace=True, subset=['mention'])\n",
    "df_pos.drop_duplicates(inplace=True, subset=['mention'])\n",
    "\n",
    "print(df_pos.shape)\n",
    "pos = IndexDataset(voc, list(df_pos['mention']), equal_length=MAX_LENGTH)\n",
    "# pos = PositiveDataset(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool_data, _ = zip(*pool)\n",
    "# pool_data = [str(item) for item in pool_data]\n",
    "# pos_data, _ = zip(*pos)\n",
    "# pos_data = [str(item) for item in pos_data]\n",
    "# len(set(pool_data).intersection(set(pos_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33539, 15)\n"
     ]
    }
   ],
   "source": [
    "df_anchor = pd.read_csv('/source/main/data_for_train/output/train/%s.csv' % POSITIVE_NAME)\n",
    "df_anchor.dropna(inplace=True, subset=['mention'])\n",
    "df_anchor.drop_duplicates(inplace=True, subset=['mention'])\n",
    "df_anchor = df_anchor.sample(df_anchor.shape[0], random_state=43)\n",
    "print(df_anchor.shape)\n",
    "anchor = IndexDataset(voc, list(df_anchor['mention']), equal_length=MAX_LENGTH)\n",
    "# anchor = PositiveDataset(anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, anchor, pos, pool):\n",
    "        super(TripletDataset, self).__init__()\n",
    "        self.anchor = anchor\n",
    "        self.pos = pos\n",
    "        self.pool = pool\n",
    "        self.len_pos = len(pos)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pool)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.anchor[idx % self.len_pos], self.pos[idx % self.len_pos], self.pool[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TripletDataset(anchor, pos, pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset=ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b√© m√¨nh u·ªëng nan __d__ nƒÉm r·ªìi , th·∫•y kh·ªèe m·∫°nh v√† ph√°t tri·ªÉn t·ªët l·∫Øm . m√¨nh ho√†n to√†n y√™n t√¢m v·ªÅ nan',\n",
       " 's·ªØa friso m√°t l·∫Øm , b√© u·ªëng th√≠ch l·∫Øm',\n",
       " 'tr·∫ßn h·ªìng qu√≠ mom c·ª© ch·ªãu kh√≥ n·∫•u , con ƒÉn ƒëc bao nhi√™u th√¨ ƒÉn th√¥i mom , ko √©p , ƒë·ªÉ cho ch∆°i nhi·ªÅu v√† gi·∫£m c·ª≠ ƒÉn v·∫∑t l√† m·∫•y ·∫£nh s·∫Ω ch·ªãu ƒÉn nhi·ªÅu h∆°n th√¥i . b√© nh√† m√¨nh c√≥ d√πng th√™m pediasure , u·ªëng ng√†y __d__ c·ª≠ th·∫•y con ƒÉn u·ªëng ngon mi·ªáng h∆°n nhi·ªÅu ·∫•y mom .']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.idx2docs(data[0].cpu().numpy()[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tr·ªôm v√≠a con ti√™u h√≥a t·ªët n√™n th√≠ch l·∫Øm v√¨ u·ªëng s·ªØa nan ƒë√≥ ch·ªã',\n",
       " 'b√© em c≈©ng d√πng friso ne , v·∫≠y m√† gi·ªù m·ªõi bi·∫øt ƒë·∫øn cu·ªôc thi',\n",
       " 'optimum m√¨nh d√πng m·∫•y h·ªôp ko tƒÉng l·∫°ng n√†o lu√¥n']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.idx2docs(data[1].cpu().numpy()[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loa loa __o__ ... aaaaaa ngon tuy·ªát v·ªùi mn ∆°iii üò≠üò≠üò≠ s·∫•u ng√¢m m·∫Øm g·ª´ng ·ªõt üòªüòªüòª b√°c e l√†m ƒë·∫£m b·∫£o s·∫°ch s·∫Ω ngon tuy·ªát v·ªùi lu√¥n ·∫•y huhu n∆∞·ªõc m·∫Øm nam ng∆∞ ƒëun l√™n , s·∫•u c≈©ng tr·∫ßn qua n∆∞·ªõc s√¥i ƒë·ªÉ k b·ªã v√°ng v√† ƒë·ªÉ ƒë∆∞·ª£c __o__ s·∫°ch s·∫Ω v·ª´a ngon lu√¥n __o__ h·ªôp ƒë·∫ßy nh∆∞ kia __d__k nha mn üòãüòãüòã k ƒÉn th√¨ s·∫Ω ti·∫øc l·∫Øm ·∫•y ... ƒÉn ng·∫•m v√†o t·∫≠n trong h·ªôt s·∫•u lu√¥n = ) ) ) ) b√°c n√†o ƒÉn b√°o e lu√¥n n√†oooo __o__ sƒët __d__ ho·∫∑c __d__',\n",
       " 'l√™ h√† trang ti·∫øc c√°i t k d√πng kotex',\n",
       " 't·∫°i sao l·∫°i nh∆∞ v·∫≠y ?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.idx2docs(data[2].cpu().numpy()[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_model = SiameseModelCore(voc.get_embedding_weights())\n",
    "model = SiameseModel(core_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE_TRAINED_MODEL='/source/main/train/output/saved_models/12bb/None.pt'\n",
    "# checkpoint = torch.load(PRE_TRAINED_MODEL, map_location=device)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13886910"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_utils.count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs):\n",
    "    model.train()\n",
    "    step_loss = model.train_batch(inputs[0], inputs[1], inputs[2])\n",
    "    return step_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3103"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# # import pdb; pdb.set_trace()\n",
    "# predict_docs(['gi√° bao ti·ªÅn', 'ee', 'Gi·∫£m gi√° s·ªëc'], batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = docs2input_tensors(['gi√° bao ti·ªÅn'], device)\n",
    "pos = docs2input_tensors(['bao nhi√™u th·∫ø ?'], device)\n",
    "neg = docs2input_tensors(['h√¥m nay t√¥i ƒëi h·ªçc'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dis_1 tensor([0.0874], device='cuda:0', grad_fn=<NormBackward1>)\n",
      "dis_2 tensor([0.2014], device='cuda:0', grad_fn=<NormBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print('dis_1', model.get_distance(anchor, pos))\n",
    "print('dis_2', model.get_distance(anchor, neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]INFO:root:\t Step: 0 Loss: 10.00150 \n",
      "100it [01:12,  1.39it/s]INFO:root:\t Step: 100 Loss: 0.72637 \n",
      "200it [02:24,  1.39it/s]INFO:root:\t Step: 200 Loss: 1.23698 \n",
      "300it [03:36,  1.39it/s]INFO:root:\t Step: 300 Loss: 0.51996 \n",
      "400it [04:48,  1.39it/s]INFO:root:\t Step: 400 Loss: 0.27985 \n",
      "500it [06:00,  1.39it/s]INFO:root:\t Step: 500 Loss: 0.49068 \n",
      "600it [07:12,  1.39it/s]INFO:root:\t Step: 600 Loss: 0.60918 \n",
      "700it [08:24,  1.39it/s]INFO:root:\t Step: 700 Loss: 0.17859 \n",
      "800it [09:36,  1.39it/s]INFO:root:\t Step: 800 Loss: 0.13675 \n",
      "900it [10:48,  1.38it/s]INFO:root:\t Step: 900 Loss: 0.14004 \n",
      "1000it [12:03,  1.39it/s]INFO:root:\t Step: 1000 Loss: 0.26835 \n",
      "1100it [13:15,  1.39it/s]INFO:root:\t Step: 1100 Loss: 0.38971 \n",
      "1200it [14:28,  1.38it/s]INFO:root:\t Step: 1200 Loss: 0.37492 \n",
      "1300it [15:40,  1.38it/s]INFO:root:\t Step: 1300 Loss: 0.22061 \n",
      "1400it [16:52,  1.39it/s]INFO:root:\t Step: 1400 Loss: 0.40308 \n",
      "1500it [18:19,  1.10s/it]INFO:root:\t Step: 1500 Loss: 0.08176 \n",
      "1600it [20:08,  1.09s/it]INFO:root:\t Step: 1600 Loss: 0.12635 \n",
      "1700it [21:27,  1.39it/s]INFO:root:\t Step: 1700 Loss: 0.20573 \n",
      "1800it [22:40,  1.38it/s]INFO:root:\t Step: 1800 Loss: 0.18790 \n",
      "1900it [24:01,  1.05s/it]INFO:root:\t Step: 1900 Loss: 0.16665 \n",
      "2000it [25:37,  1.39it/s]INFO:root:\t Step: 2000 Loss: 0.22034 \n",
      "2065it [26:24,  1.39it/s]"
     ]
    }
   ],
   "source": [
    "model.build_stuff_for_training(device)\n",
    "for epoch_idx in range(1):\n",
    "    start = time.time()\n",
    "    for idx, inputs in tqdm(enumerate(data_loader)):\n",
    "        inputs = [i.to(device) for i in inputs]\n",
    "        l = train_step(inputs)\n",
    "        if idx % 100 == 0:\n",
    "            logging.info('\\t Step: %s Loss: %.5f ', idx, l)\n",
    "    duration = time.time() - start\n",
    "    logging.info('Epoch %s took %.2f s', epoch_idx, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch_idx in range(10):\n",
    "#     start = time.time()\n",
    "#     model.train()\n",
    "#     for idx, inputs in tqdm(enumerate(data_loader)):    \n",
    "#         inputs = [i.to(device) for i in inputs]\n",
    "#         l = train_step(inputs)\n",
    "#     duration = time.time() - start\n",
    "#     logging.info('Epoch %s took %.2f s', epoch_idx, duration)\n",
    "    \n",
    "#     model.eval()    \n",
    "#     df_pos['pred'] = predict_docs(df_pos['mention'], batch_size=256)\n",
    "#     df_pool['pred'] = predict_docs(df_pool['mention'], batch_size=256)\n",
    "    \n",
    "#     logging.info('Recall: %s/%s=%.4f', (df_pos['pred']>=0.5).sum(), df_pos.shape[0], \n",
    "#                  (df_pos['pred']>=0.5).sum()/df_pos.shape[0])\n",
    "#     logging.info('Ratio on pool: %s/%s=%.4f', (df_pool['pred']>=0.5).sum(), df_pool.shape[0], \n",
    "#                  (df_pool['pred']>=0.5).sum()/df_pool.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "# ax = fig.add_subplot(1, 2, 1)\n",
    "# df_pos.loc[:500, 'pred'].hist(bins=100, ax=ax)\n",
    "# ax.set_title('Spy')\n",
    "# ax.set_xlim(0, 0.9)\n",
    "# ax.set_ylim(0, 100)\n",
    "\n",
    "# ax = fig.add_subplot(1, 2, 2)\n",
    "# df_pos.loc[500:, 'pred'].hist(bins=100, ax=ax)\n",
    "# ax.set_title('Positive')\n",
    "# ax.set_xlim(0, 0.9)\n",
    "# ax.set_ylim(0, 100)\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pool_social = pd.read_csv('/source/main/data_for_train/output/train/pool.csv')\n",
    "# df_pool_social.rename(columns={'target': 'mention'}, inplace=True)\n",
    "# df_pool_social.dropna(inplace=True, subset=['mention'])\n",
    "# df_pool_social.drop_duplicates(inplace=True, subset=['mention'])\n",
    "# df_pool_social = df_pool_social.iloc[:794323, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_checker = TrainingChecker(model, root_dir='/source/main/train/output/saved_models/%s/' % EXP_ID,\n",
    "                                   init_score=-10000)\n",
    "training_checker.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = docs2input_tensors([df_pos['mention'].iloc[20]], device)\n",
    "pos = docs2input_tensors([df_pos['mention'].iloc[40]], device)\n",
    "neg = docs2input_tensors(['h√¥m nay t√¥i ƒëi h·ªçc'], device)\n",
    "print('dis_1', model.get_distance(anchor, pos))\n",
    "print('dis_2', model.get_distance(anchor, neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos['mention'].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS = list(df_pos['mention'].sample(5))\n",
    "\n",
    "def get_distance(docs_1, docs_2):\n",
    "    with torch.no_grad():\n",
    "        docs1 = docs2input_tensors(docs_1, device)\n",
    "        docs2 = docs2input_tensors(docs_2, device)\n",
    "        return model.get_distance(docs1, docs2).cpu().numpy()\n",
    "\n",
    "def predict_batch(docs):\n",
    "    len_docs = len(docs)\n",
    "    pos_docs = [item for doc in POS for item in [doc]*len_docs]\n",
    "    docs = docs * len(POS)\n",
    "    \n",
    "    dis = np.array(get_distance(pos_docs, docs))\n",
    "    dis = dis.reshape((len(POS), len_docs))\n",
    "#     print(dis)\n",
    "    return dis.mean(axis=0)\n",
    "\n",
    "# def get_distance_to_pos(docs, batch_size):\n",
    "#     return list(chain(*[predict_batch(docs[i: i+batch_size]) for i in tqdm(range(0, len(docs), batch_size))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predict_docs(['gi√° bao ti·ªÅn', 'ee', 'Gi·∫£m gi√° s·ªëc'], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from sklearn import metrics\n",
    "\n",
    "from data_for_train.index_dataset import IndexDataset\n",
    "from data_for_train.positive_dataset import PositiveDataset\n",
    "from data_for_train import pool\n",
    "from naruto_skills.new_voc import Voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos['pred'] = predict_docs(list(df_pos['mention']), batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos['pred'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = df_pos['pred'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_eval = pd.read_csv('/source/main/data_for_train/output/eval/%s.csv' % POSITIVE_NAME)\n",
    "df_pos_eval = df_pos_eval.drop_duplicates(subset=['mention'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_eval['mention'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_eval.dropna(subset=['mention'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_eval['pred'] = predict_docs(list(df_pos_eval['mention']), batch_size=256)\n",
    "# print(sum(df_pos_eval['pred']>=0.5)/df_pos_eval.shape[0])\n",
    "# print(df_pos_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_pos_eval['pred']<=THRESHOLD).sum()/df_pos_eval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_test = pd.read_csv('/source/main/data_for_train/output/test/%s.csv' % POSITIVE_NAME)\n",
    "df_pos_test = df_pos_test.drop_duplicates(subset=['mention'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_test['pred'] = predict_docs(list(df_pos_test['mention']), batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(df_pos_test['pred']<=THRESHOLD)/df_pos_test.shape[0])\n",
    "print(df_pos_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score: pr/P(y=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool_eval = pd.read_csv('/source/main/data_for_train/output/eval/pool.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool_eval['pred'] = predict_docs(list(df_pool_eval['mention']), batch_size=256)\n",
    "\n",
    "# print(sum(df_pool_eval['pred']>=0.5)/df_pool_eval.shape[0])\n",
    "# print(df_pool_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_pool_eval['pred']<=THRESHOLD)/df_pool_eval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pool_eval['pred2'] = predict_docs(list(df_pool_eval['mention']), batch_size=64)\n",
    "# sum(df_pool_eval['pred2'] <= 5.871464)/df_pool_eval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(df_pool_eval['pred2'] <= 5.871464)/df_pool_eval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool_eval[df_pool_eval['pred']<=THRESHOLD].sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool_test = pd.read_csv('/source/main/data_for_train/output/test/pool.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool_test['pred'] = predict_docs(list(df_pool_test['mention']), batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(df_pool_test['pred']<=THRESHOLD)/df_pool_test.shape[0])\n",
    "print(df_pool_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pos_pred = sum(df_pool_test['pred']<=THRESHOLD)\n",
    "df_pool_test[df_pool_test['pred']<=THRESHOLD].sample(min(100, total_pos_pred)).to_csv('%s.csv' % EXP_ID, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pos_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos[['mention']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool_test[df_pool_test['pred']<=THRESHOLD].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
